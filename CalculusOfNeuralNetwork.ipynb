{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cc45a38-b04c-43fc-a3eb-d7a9fee3da06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  500 | loss 0.002250\n",
      "Epoch 1000 | loss 0.000817\n",
      "Epoch 1500 | loss 0.000479\n",
      "Epoch 2000 | loss 0.000334\n",
      "Epoch 2500 | loss 0.000255\n",
      "Epoch 3000 | loss 0.000205\n",
      "Epoch 3500 | loss 0.000171\n",
      "Epoch 4000 | loss 0.000146\n",
      "Epoch 4500 | loss 0.000127\n",
      "Epoch 5000 | loss 0.000113\n",
      "x=(0.0,0.0) -> y=(0.987,0.013)  class=0\n",
      "x=(0.0,1.0) -> y=(0.003,0.997)  class=1\n",
      "x=(1.0,0.0) -> y=(0.998,0.002)  class=0\n",
      "x=(1.0,1.0) -> y=(0.987,0.013)  class=0\n",
      "x=(0.2,0.8) -> y=(0.018,0.982)  class=1\n",
      "x=(0.8,0.2) -> y=(0.998,0.002)  class=0\n"
     ]
    }
   ],
   "source": [
    "#The calculus of a simple neural network with one hidden layer\n",
    "import math\n",
    "import random\n",
    "\n",
    "# defining sigmoid and its derivative\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + math.exp(-z))\n",
    "\n",
    "def dsigmoid(y):\n",
    "    # derivative w.r.t. z, when given y = sigmoid(z)\n",
    "    return y * (1.0 - y)\n",
    "\n",
    "# ----- Tiny dataset (2D input -> 2D one-hot target) -----\n",
    "# We'll learn a simple decision boundary:\n",
    "# class A (target [1,0]) when x1 >= x2, else class B (target [0,1])\n",
    "data = [\n",
    "    # (x1, x2), (t1, t2)\n",
    "    ((0.0, 0.0), (1.0, 0.0)),\n",
    "    ((0.0, 1.0), (0.0, 1.0)),\n",
    "    ((1.0, 0.0), (1.0, 0.0)),\n",
    "    ((1.0, 1.0), (1.0, 0.0)),\n",
    "    ((0.2, 0.8), (0.0, 1.0)),\n",
    "    ((0.8, 0.2), (1.0, 0.0)),\n",
    "]\n",
    "\n",
    "# Initialize parameters\n",
    "def rand():\n",
    "    return random.uniform(-0.5, 0.5)\n",
    "\n",
    "w1, w2, b1 = rand(), rand(), rand()\n",
    "w3, w4, b2 = rand(), rand(), rand()\n",
    "w5, w6, b3 = rand(), rand(), rand()\n",
    "w7, w8, b4 = rand(), rand(), rand()\n",
    "\n",
    "lr = 0.5          # learning rate\n",
    "epochs = 5000     # iterations\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    random.shuffle(data)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for (x1, x2), (t1, t2) in data:\n",
    "        # Forward pass\n",
    "        # Hidden layer\n",
    "        z1 = w1*x1 + w2*x2 + b1\n",
    "        h1 = sigmoid(z1)\n",
    "        z2 = w3*x1 + w4*x2 + b2\n",
    "        h2 = sigmoid(z2)\n",
    "\n",
    "        # Output layer\n",
    "        z3 = w5*h1 + w6*h2 + b3\n",
    "        y1 = sigmoid(z3)\n",
    "        z4 = w7*h1 + w8*h2 + b4\n",
    "        y2 = sigmoid(z4)\n",
    "\n",
    "        # Loss = 1/2 * [(t1 - y1)^2 + (t2 - y2)^2]\n",
    "        L = 0.5*((t1 - y1)**2 + (t2 - y2)**2)\n",
    "        total_loss += L\n",
    "\n",
    "        # Backward pass\n",
    "        # Output layer gradients\n",
    "        dL_dy1 = -(t1 - y1)\n",
    "        dL_dy2 = -(t2 - y2)\n",
    "\n",
    "        dy1_dz3 = dsigmoid(y1)\n",
    "        dy2_dz4 = dsigmoid(y2)\n",
    "\n",
    "        # Gradients for w5, w6, b3 (affecting y1)\n",
    "        dL_dw5 = dL_dy1 * dy1_dz3 * h1\n",
    "        dL_dw6 = dL_dy1 * dy1_dz3 * h2\n",
    "        dL_db3 = dL_dy1 * dy1_dz3 * 1.0\n",
    "\n",
    "        # Gradients for w7, w8, b4 (affecting y2)\n",
    "        dL_dw7 = dL_dy2 * dy2_dz4 * h1\n",
    "        dL_dw8 = dL_dy2 * dy2_dz4 * h2\n",
    "        dL_db4 = dL_dy2 * dy2_dz4 * 1.0\n",
    "\n",
    "        # Hidden layer error terms (sum of paths from both outputs)\n",
    "        dL_dh1 = (dL_dy1 * dy1_dz3 * w5) + (dL_dy2 * dy2_dz4 * w7)\n",
    "        dL_dh2 = (dL_dy1 * dy1_dz3 * w6) + (dL_dy2 * dy2_dz4 * w8)\n",
    "\n",
    "        dh1_dz1 = dsigmoid(h1)\n",
    "        dh2_dz2 = dsigmoid(h2)\n",
    "\n",
    "        # Gradients for w1, w2, b1 (h1 branch)\n",
    "        dL_dw1 = dL_dh1 * dh1_dz1 * x1\n",
    "        dL_dw2 = dL_dh1 * dh1_dz1 * x2\n",
    "        dL_db1 = dL_dh1 * dh1_dz1 * 1.0\n",
    "\n",
    "        # Gradients for w3, w4, b2 (h2 branch)\n",
    "        dL_dw3 = dL_dh2 * dh2_dz2 * x1\n",
    "        dL_dw4 = dL_dh2 * dh2_dz2 * x2\n",
    "        dL_db2 = dL_dh2 * dh2_dz2 * 1.0\n",
    "\n",
    "        # Gradient descent updates on weights\n",
    "        w5 -= lr * dL_dw5\n",
    "        w6 -= lr * dL_dw6\n",
    "        b3 -= lr * dL_db3\n",
    "\n",
    "        w7 -= lr * dL_dw7\n",
    "        w8 -= lr * dL_dw8\n",
    "        b4 -= lr * dL_db4\n",
    "\n",
    "        w1 -= lr * dL_dw1\n",
    "        w2 -= lr * dL_dw2\n",
    "        b1 -= lr * dL_db1\n",
    "\n",
    "        w3 -= lr * dL_dw3\n",
    "        w4 -= lr * dL_dw4\n",
    "        b2 -= lr * dL_db2\n",
    "\n",
    "    if (epoch+1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch+1:4d} | loss {total_loss/len(data):.6f}\")\n",
    "\n",
    "# test after training\n",
    "def predict(x1, x2):\n",
    "    z1 = w1*x1 + w2*x2 + b1\n",
    "    h1 = sigmoid(z1)\n",
    "    z2 = w3*x1 + w4*x2 + b2\n",
    "    h2 = sigmoid(z2)\n",
    "    z3 = w5*h1 + w6*h2 + b3\n",
    "    y1 = sigmoid(z3)\n",
    "    z4 = w7*h1 + w8*h2 + b4\n",
    "    y2 = sigmoid(z4)\n",
    "    return y1, y2\n",
    "\n",
    "tests = [(0.0,0.0),(0.0,1.0),(1.0,0.0),(1.0,1.0),(0.2,0.8),(0.8,0.2)]\n",
    "for x1,x2 in tests:\n",
    "    y1,y2 = predict(x1,x2)\n",
    "    pred = 0 if y1 > y2 else 1\n",
    "    print(f\"x=({x1:.1f},{x2:.1f}) -> y=({y1:.3f},{y2:.3f})  class={pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b717c0-27cf-48f2-9951-23f31e87a333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.4989209769728176\n",
      "Error: 0.4928010095250592\n",
      "Error: 0.4524517814135708\n",
      "Error: 0.33737448122748165\n",
      "Error: 0.18425789901307968\n",
      "Error: 0.11117311085940629\n",
      "Error: 0.0800143863681107\n",
      "Error: 0.06360704899926437\n",
      "Error: 0.05352719284732321\n",
      "Error: 0.04667937944525244\n",
      "Output after training:\n",
      "[[0.02955785]\n",
      " [0.95947811]\n",
      " [0.95450775]\n",
      " [0.05123815]]\n"
     ]
    }
   ],
   "source": [
    "#Matrix based learning approach (comp\n",
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Input dataset\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "# Output dataset\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0]\n",
    "])\n",
    "\n",
    "# Initialize weights and biases with random values\n",
    "input_neurons = 2\n",
    "hidden_neurons = 10\n",
    "output_neurons = 1\n",
    "\n",
    "hidden_weights = np.random.uniform(size=(input_neurons, hidden_neurons))\n",
    "hidden_bias = np.random.uniform(size=(1, hidden_neurons))\n",
    "\n",
    "output_weights = np.random.uniform(size=(hidden_neurons, output_neurons))\n",
    "output_bias = np.random.uniform(size=(1, output_neurons))\n",
    "\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Training the neural network\n",
    "for epoch in range(epochs):\n",
    "    # Forward propagation\n",
    "    # Input to Hidden Layer\n",
    "    hidden_layer_activation = np.dot(X, hidden_weights) + hidden_bias\n",
    "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "\n",
    "    # Hidden to Output Layer\n",
    "    output_layer_activation = np.dot(hidden_layer_output, output_weights) + output_bias\n",
    "    predicted_output = sigmoid(output_layer_activation)\n",
    "\n",
    "    # Backpropagation\n",
    "    # Calculate the error\n",
    "    error = y - predicted_output\n",
    "\n",
    "    # Compute the gradients for output layer\n",
    "    output_delta = error * sigmoid_derivative(predicted_output)\n",
    "    hidden_layer_error = output_delta.dot(output_weights.T)\n",
    "    hidden_delta = hidden_layer_error * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "    # Update weights and biases\n",
    "    output_weights += hidden_layer_output.T.dot(output_delta) * learning_rate\n",
    "    output_bias += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
    "    hidden_weights += X.T.dot(hidden_delta) * learning_rate\n",
    "    hidden_bias += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Error: {np.mean(np.abs(error))}\")\n",
    "\n",
    "print(\"Output after training:\")\n",
    "print(predicted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002592ed-1330-45f0-bf83-184d74461e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
